#!/usr/bin/env python3
"""
apply_fixes.py - Applies code fixes suggested by Claude AI

This script:
1. Reads the fixes from the Claude AI response
2. Parses each fix to identify file paths and code changes
3. Validates the fixes for common errors (like unmatched braces)
4. Applies the changes to the appropriate files with smart matching
5. Validates the changes to ensure they don't break the code

Optimized version with improved performance, smart matching, and better reporting.
"""

import json
import os
import re
import subprocess
import sys
import argparse
import tempfile
import difflib
from pathlib import Path
from time import time
from collections import defaultdict

# Maximum number of fixes to apply per file to avoid large-scale changes
MAX_FIXES_PER_FILE = 10

# Maximum percentage of file that can be changed (to avoid destructive changes)
# Can be overridden with --force or --max-change-percentage
MAX_CHANGE_PERCENTAGE = 30

# How many fixes to apply before running cargo check in incremental mode
VALIDATION_INTERVAL = 3

# Threshold for fuzzy matching (0-100, higher = more strict)
FUZZY_MATCH_THRESHOLD = 85

class FixStats:
    """Track statistics about applied fixes"""
    def __init__(self):
        self.attempted = 0
        self.exact_match = 0
        self.fuzzy_match = 0
        self.skipped = 0
        self.force_applied = 0
        self.validation_failures = 0
        self.files_affected = set()
        self.last_validation_time = 0
        self.total_validation_time = 0

    def get_report(self):
        """Generate a text report of the statistics"""
        report = "\n=== Summary Report ===\n"
        report += f"- Fixes Attempted: {self.attempted}\n"
        report += f"- Fixes Applied: {self.exact_match + self.fuzzy_match} "
        report += f"({self.exact_match} exact, {self.fuzzy_match} fuzzy match)\n"
        report += f"- Fixes Skipped: {self.skipped} (no suitable match)\n"
        report += f"- Force Applied Fixes: {self.force_applied} (manual override)\n"
        report += f"- Validation Failures: {self.validation_failures}\n"
        report += f"- Files Affected: {len(self.files_affected)}\n"
        
        if self.last_validation_time > 0:
            report += f"- Last Validation Duration: {self.last_validation_time:.2f}s\n"
        if self.total_validation_time > 0:
            report += f"- Total Validation Time: {self.total_validation_time:.2f}s\n"
            
        return report

def parse_fixes_file(file_path='claude_fixes.json'):
    """Parse the fixes file generated by Claude AI."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('fixes', [])
    except FileNotFoundError:
        print(f"Warning: {file_path} file not found")
        return []
    except json.JSONDecodeError:
        print(f"Error: {file_path} is not valid JSON")
        return []
    except Exception as e:
        print(f"Error reading fixes file: {e}")
        return []

def parse_fix_block(fix_text):
    """Parse a single fix block."""
    lines = fix_text.strip().split('\n')
    
    fix_info = {
        'file': None,
        'original': [],
        'fixed': [],
        'explanation': [],
        'current_section': None
    }
    
    for line in lines:
        # File path is specified with "file: path/to/file.rs"
        if line.startswith('file:'):
            fix_info['file'] = line[5:].strip()
        # Start of original code block
        elif line == 'original: |':
            fix_info['current_section'] = 'original'
        # Start of fixed code block
        elif line == 'fixed: |':
            fix_info['current_section'] = 'fixed'
        # Start of explanation block
        elif line == 'explanation: |':
            fix_info['current_section'] = 'explanation'
        # Separator between sections
        elif line == '---':
            fix_info['current_section'] = None
        # Content for the current section
        elif fix_info['current_section']:
            # Remove the leading two spaces if they exist (YAML block scalar indentation)
            if line.startswith('  '):
                line = line[2:]
            fix_info[fix_info['current_section']].append(line)
    
    # Join the content of each section into a single string
    for section in ['original', 'fixed', 'explanation']:
        fix_info[section] = '\n'.join(fix_info[section])
    
    # Clean up
    del fix_info['current_section']
    
    return fix_info

def check_brace_balance(code_snippet):
    """
    Check if braces, parentheses, and brackets are properly balanced and look for risky patterns.
    Returns (is_balanced, issue_description)
    """
    # Check for consecutive delimiters which are often an issue
    danger_patterns = [
        (r'}\s*}', "consecutive closing braces"),
        (r'{\s*{', "consecutive opening braces"),
        (r'\)\s*\)', "consecutive closing parentheses"),
        (r'\(\s*\(', "consecutive opening parentheses"),
        (r'\]\s*\]', "consecutive closing brackets"),
        (r'\[\s*\[', "consecutive opening brackets")
    ]
    
    # Unified check for dangerous patterns
    for pattern, description in danger_patterns:
        if re.search(pattern, code_snippet):
            return False, f"Contains {description}"
    
    # Check for balanced delimiters
    stack = []
    opening = {'(': ')', '{': '}', '[': ']'}
    closing = {')': '(', '}': '{', ']': '['}
    
    for i, char in enumerate(code_snippet):
        if char in opening:
            stack.append((char, i))
        elif char in closing:
            if not stack or stack[-1][0] != closing[char]:
                return False, f"Unmatched closing delimiter '{char}' at position {i}"
            stack.pop()
    
    if stack:
        return False, f"Unmatched opening delimiter '{stack[-1][0]}' at position {stack[-1][1]}"
    
    return True, "Balanced"

def validate_fix(fix):
    """Validate that a fix has all required components and passes basic checks."""
    required_fields = ['file', 'original', 'fixed']
    for field in required_fields:
        if not fix.get(field):
            print(f"Warning: Missing required field '{field}' in fix")
            return False
    
    # Ensure file exists
    if not os.path.isfile(fix['file']):
        print(f"Warning: File {fix['file']} does not exist")
        return False
    
    # Check if original and fixed are identical
    if fix['original'] == fix['fixed']:
        print(f"Warning: Original and fixed code are identical for {fix['file']}")
        return False
    
    # Validate brace balance in the fixed code
    is_balanced, message = check_brace_balance(fix['fixed'])
    if not is_balanced:
        print(f"Warning: Fixed code has unbalanced delimiters in {fix['file']}: {message}")
        print("This would cause syntax errors and has been rejected.")
        return False
    
    return True

def similarity_ratio(a, b):
    """Calculate similarity between two strings (0-100)"""
    return difflib.SequenceMatcher(None, a, b).ratio() * 100

def find_function_scope(code, snippet):
    """
    Try to identify the function or code block containing the snippet.
    Returns the full function text if found, None otherwise.
    """
    # Common Rust function patterns
    patterns = [
        # pub fn name(...) { ... }
        r'pub\s+fn\s+[a-zA-Z0-9_]+\s*\([^{]*\)\s*(?:->[^{]*)?\s*\{(?:[^{}]|(?R))*\}',
        # fn name(...) { ... }
        r'fn\s+[a-zA-Z0-9_]+\s*\([^{]*\)\s*(?:->[^{]*)?\s*\{(?:[^{}]|(?R))*\}',
        # pub struct Name { ... }
        r'pub\s+struct\s+[a-zA-Z0-9_]+\s*\{(?:[^{}]|(?R))*\}',
        # struct Name { ... }
        r'struct\s+[a-zA-Z0-9_]+\s*\{(?:[^{}]|(?R))*\}',
        # impl ... { ... }
        r'impl\s+(?:[^{]*)\s*\{(?:[^{}]|(?R))*\}',
        # trait ... { ... }
        r'trait\s+[a-zA-Z0-9_]+\s*(?:<[^>]*>)?\s*\{(?:[^{}]|(?R))*\}'
    ]
    
    # Try to find key identifiers in the snippet
    identifiers = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', snippet)
    
    # Keep identifiers that seem to be function names or type names (more likely to be unique)
    function_names = [id for id in identifiers if len(id) > 3 and not id in [
        'self', 'super', 'true', 'false', 'Some', 'None', 'Error', 'Result',
        'Option', 'match', 'while', 'break', 'continue', 'return', 'struct',
        'enum', 'trait', 'impl', 'where', 'type', 'const', 'static'
    ]]
    
    # Try to find the functions containing these identifiers
    candidate_blocks = []
    
    # First, try to match by identifiers
    if function_names:
        for name in function_names:
            # Look for function containing this name
            for pattern in patterns:
                func_matches = re.finditer(pattern, code, re.DOTALL)
                for match in func_matches:
                    func_text = match.group(0)
                    if name in func_text and snippet in func_text:
                        return func_text
            
            # If no match through regex, try a simpler approach - find the enclosing block
            if name in code:
                lines = code.split('\n')
                for i, line in enumerate(lines):
                    if name in line:
                        # Look backwards for the start of a function
                        start_idx = None
                        for j in range(i, -1, -1):
                            if 'fn ' in lines[j] or 'struct ' in lines[j] or 'impl ' in lines[j]:
                                start_idx = j
                                break
                        
                        if start_idx is not None:
                            # Look forwards for the end of a block
                            brace_count = 0
                            end_idx = None
                            for j in range(start_idx, len(lines)):
                                brace_count += lines[j].count('{') - lines[j].count('}')
                                if brace_count == 0 and j > start_idx:
                                    end_idx = j
                                    break
                            
                            if end_idx is not None:
                                block = '\n'.join(lines[start_idx:end_idx+1])
                                if snippet in block:
                                    return block
    
    # If no match by identifiers, try each regex pattern directly
    for pattern in patterns:
        try:
            func_matches = re.finditer(pattern, code, re.DOTALL)
            for match in func_matches:
                func_text = match.group(0)
                if snippet in func_text:
                    return func_text
        except Exception as e:
            # Complex regex can sometimes cause backtracking issues in large files
            pass
    
    return None

def smartly_apply_fix(file_content, original, fixed, file_path, force=False):
    """
    Apply fix with smart matching, returning (updated_content, match_type) where
    match_type is one of: 'exact', 'fuzzy', 'skipped', 'force'
    """
    # 1. Try exact match first
    if original in file_content:
        return file_content.replace(original, fixed), 'exact'
    
    # 2. Try normalizing whitespace
    normalized_original = re.sub(r'\s+', ' ', original.strip())
    normalized_content = re.sub(r'\s+', ' ', file_content.strip())
    
    # If normalization helps, use regex replacement
    if normalized_original in normalized_content:
        pattern = re.escape(normalized_original).replace('\\ ', r'\s+')
        try:
            # Only do this if the change is relatively minor
            change_percent = (len(normalized_original) / len(normalized_content)) * 100
            if change_percent <= 10:
                updated_content = re.sub(pattern, fixed, file_content)
                return updated_content, 'fuzzy'
        except Exception as e:
            print(f"Error applying regex replacement: {e}")
    
    # 3. Try fuzzy matching based on function scope
    # Find the function or block containing this code
    func_scope = find_function_scope(file_content, original.strip())
    
    if func_scope:
        # Calculate similarity percentage
        sim_percent = similarity_ratio(func_scope, original)
        print(f"Found potential function scope with {sim_percent:.1f}% similarity")
        
        if sim_percent >= FUZZY_MATCH_THRESHOLD:
            # If similarity is high, apply the change within the function scope
            updated_func = func_scope.replace(original, fixed)
            
            # Only apply if the change was actually made
            if updated_func != func_scope:
                updated_content = file_content.replace(func_scope, updated_func)
                print(f"Applying fuzzy match for {file_path} (similarity: {sim_percent:.1f}%)")
                return updated_content, 'fuzzy'
    
    # 4. Try line-by-line matching
    original_lines = original.strip().split('\n')
    content_lines = file_content.strip().split('\n')
    
    # Only try this for relatively small blocks
    if len(original_lines) <= 15:
        # Find best matching section
        best_match_start = -1
        best_match_score = 0
        
        for i in range(len(content_lines) - len(original_lines) + 1):
            section = '\n'.join(content_lines[i:i+len(original_lines)])
            score = similarity_ratio(section, original)
            if score > best_match_score:
                best_match_score = score
                best_match_start = i
        
        if best_match_score >= FUZZY_MATCH_THRESHOLD:
            # Replace the matched section
            matched_section = '\n'.join(content_lines[best_match_start:best_match_start+len(original_lines)])
            
            print(f"Found line-by-line match with {best_match_score:.1f}% similarity")
            print(f"Applying fuzzy match for {file_path} (similarity: {best_match_score:.1f}%)")
            
            updated_content = file_content.replace(matched_section, fixed)
            return updated_content, 'fuzzy'
    
    # If force mode and we found any partial match
    if force and func_scope:
        updated_func = func_scope.replace(original, fixed)
        # Only apply if the change was actually made
        if updated_func != func_scope:
            updated_content = file_content.replace(func_scope, updated_func)
            print(f"WARNING: Force-applying match for {file_path} despite low similarity")
            return updated_content, 'force'
    
    # If we couldn't apply the fix
    print(f"Could not find a suitable match for applying fix to {file_path}")
    return file_content, 'skipped'

def validate_rust_syntax(file_path, stats=None):
    """Validate Rust file syntax using cargo check."""
    print(f"Validating syntax of {file_path}...")
    
    try:
        # Time the validation
        start_time = time()
        
        # Use cargo check to validate the entire project
        result = subprocess.run(
            ['cargo', 'check', '--quiet'],
            capture_output=True,
            text=True
        )
        
        # Record the time taken
        end_time = time()
        duration = end_time - start_time
        
        if stats:
            stats.last_validation_time = duration
            stats.total_validation_time += duration
        
        if result.returncode != 0:
            print(f"Error: Project validation failed for changes in {file_path}")
            print(result.stderr)
            return False
    except Exception as e:
        print(f"Error during validation: {e}")
        return False
    
    return True

def validate_single_file(file_path, stats=None):
    """Validate a single Rust file for syntax correctness."""
    print(f"Validating {file_path}...")
    
    # Use our improved validate_rust_syntax function
    return validate_rust_syntax(file_path, stats)

def validate_project(stats=None):
    """Validate the entire project with cargo check."""
    print("Running cargo check to validate all changes...")
    
    start_time = time()
    
    result = subprocess.run(['cargo', 'check'], 
                         capture_output=True, 
                         text=True)
    
    # Record the time taken
    end_time = time()
    duration = end_time - start_time
    
    if stats:
        stats.last_validation_time = duration
        stats.total_validation_time += duration
    
    if result.returncode != 0:
        print("Error: Changes caused compilation errors:")
        print(result.stderr)
        
        print("Details of errors:")
        for line in result.stderr.split('\n'):
            if "error" in line:
                print(f"  {line}")
        
        return False
    
    print("Project validation successful!")
    return True

def apply_fix_to_file(fix, applied_fixes, stats, dry_run=False, force=False, max_change_percentage=None):
    """Apply a single fix to a file."""
    if not validate_fix(fix):
        return False
    
    file_path = fix['file']
    original = fix['original']
    fixed = fix['fixed']
    
    # Increment attempted count
    stats.attempted += 1
    
    # Determine the max change percentage to use
    effective_max_change = MAX_CHANGE_PERCENTAGE
    if max_change_percentage is not None:
        effective_max_change = max_change_percentage
    
    # Check if we've already applied the maximum number of fixes to this file
    if file_path in applied_fixes and applied_fixes[file_path] >= MAX_FIXES_PER_FILE:
        print(f"Skipping fix for {file_path}: maximum number of fixes ({MAX_FIXES_PER_FILE}) already applied")
        stats.skipped += 1
        return False
    
    # Read file content
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading file {file_path}: {e}")
        stats.skipped += 1
        return False
    
    # Calculate change percentage
    def calculate_change_percentage(old_text, new_text):
        import difflib
        diff = difflib.unified_diff(
            old_text.splitlines(keepends=True),
            new_text.splitlines(keepends=True)
        )
        changes = sum(1 for line in diff if line.startswith('+') or line.startswith('-'))
        total_lines = len(old_text.splitlines())
        if total_lines == 0:
            return 100
        return (changes / total_lines) * 100 / 2  # Divide by 2 as each change counts twice (+ and -)
    
    # Apply the fix using smart matching
    updated_content, match_type = smartly_apply_fix(content, original, fixed, file_path, force)
    
    # If the content didn't change, no need to continue
    if content == updated_content:
        print(f"No changes needed for {file_path}")
        stats.skipped += 1
        return False
    
    # Update stats based on match type
    if match_type == 'exact':
        stats.exact_match += 1
    elif match_type == 'fuzzy':
        stats.fuzzy_match += 1
    elif match_type == 'force':
        stats.force_applied += 1
    elif match_type == 'skipped':
        stats.skipped += 1
        return False
    
    # Change percentage check
    change_percent = calculate_change_percentage(content, updated_content)
    if change_percent > effective_max_change and not force:
        print(f"Warning: Change too large ({change_percent:.1f}%) for {file_path}, skipping")
        print(f"Use --force to apply changes regardless of size")
        stats.skipped += 1
        return False
    
    # Final brace balance check on the updated content
    is_balanced, message = check_brace_balance(updated_content)
    if not is_balanced:
        print(f"ERROR: Updated content would have unbalanced delimiters in {file_path}: {message}")
        print("This would cause syntax errors and has been rejected.")
        stats.skipped += 1
        return False
    
    if dry_run:
        print(f"Would update {file_path} (change: {change_percent:.1f}%, match type: {match_type})")
        return True
    
    # Create a backup of the original file
    backup_path = f"{file_path}.bak"
    with open(backup_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    # Apply the changes
    try:
        # Write the updated content
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        # Update applied fixes count
        if file_path not in applied_fixes:
            applied_fixes[file_path] = 1
        else:
            applied_fixes[file_path] += 1
        
        # Update stats
        stats.files_affected.add(file_path)
        
        print(f"Updated {file_path} (applied fix {applied_fixes[file_path]}/{MAX_FIXES_PER_FILE}, match type: {match_type})")
        print(f"Backup saved to {backup_path}")
        return True
    except Exception as e:
        print(f"Error writing to file {file_path}: {e}")
        stats.skipped += 1
        return False

def revert_file(file_path):
    """Revert a file to its backup version."""
    backup_path = f"{file_path}.bak"
    if os.path.exists(backup_path):
        try:
            os.rename(backup_path, file_path)
            print(f"Reverted changes to {file_path}")
            return True
        except Exception as e:
            print(f"Error reverting {file_path}: {e}")
            print(f"Manual restore needed from {backup_path}")
            return False
    else:
        print(f"No backup found for {file_path}")
        return False

def main():
    # Declare the global variable before using it
    global FUZZY_MATCH_THRESHOLD
    
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Apply fixes from Claude AI')
    parser.add_argument('--force', action='store_true', 
                        help='Force applying fixes regardless of change size')
    parser.add_argument('--max-change', type=float, dest='max_change_percentage', default=25.0,
                        help=f'Maximum percentage of file that can be changed (default: 25%%)')
    parser.add_argument('--fixes-file', type=str, default='claude_fixes.json',
                        help='Path to the JSON file containing fixes (default: claude_fixes.json)')
    parser.add_argument('--no-interactive', action='store_true',
                        help='Run in non-interactive mode (no prompts)')
    parser.add_argument('--no-validation', action='store_true',
                        help='Skip validation of changes')
    parser.add_argument('--incremental', action='store_true',
                        help='Apply and validate fixes incrementally (one at a time)')
    parser.add_argument('--batch', action='store_true',
                        help='Apply all fixes at once (less safe than incremental mode)')
    parser.add_argument('--validation-interval', type=int, default=VALIDATION_INTERVAL,
                        help=f'Number of fixes to apply before validating in incremental mode (default: {VALIDATION_INTERVAL})')
    parser.add_argument('--fuzzy-threshold', type=float, default=FUZZY_MATCH_THRESHOLD,
                        help=f'Threshold percentage for fuzzy matching (default: {FUZZY_MATCH_THRESHOLD})')
    args = parser.parse_args()
    
    # Assign the parsed value after defining the variable globally
    FUZZY_MATCH_THRESHOLD = args.fuzzy_threshold
    
    # Stats tracking
    stats = FixStats()
    
    # Parse the fixes
    raw_fixes = parse_fixes_file(args.fixes_file)
    
    if not raw_fixes:
        print("No fixes to apply")
        # Write to changes_count.txt for GitHub Actions
        with open('changes_count.txt', 'w') as f:
            f.write("0")
        
        # Use the newer GitHub Actions output method
        if 'GITHUB_OUTPUT' in os.environ:
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write(f"has_changes=0\n")
                f.write(f"changes_valid=false\n")
        sys.exit(0)
    
    print(f"Found {len(raw_fixes)} potential fixes to apply")
    
    # Parse each fix block
    fixes = []
    for fix_text in raw_fixes:
        try:
            fix = parse_fix_block(fix_text)
            fixes.append(fix)
        except Exception as e:
            print(f"Error parsing fix: {e}")
    
    print(f"Successfully parsed {len(fixes)} fixes")
    
    # Group fixes by file
    fixes_by_file = {}
    for fix in fixes:
        if not fix.get('file'):
            continue
        file_path = fix['file']
        if file_path not in fixes_by_file:
            fixes_by_file[file_path] = []
        fixes_by_file[file_path].append(fix)
    
    # Apply the fixes
    applied_fixes = {}  # Track how many fixes we've applied to each file
    changed_files = set()  # Track which files we've changed
    failed_fixes = []  # Track fixes that failed to apply or validate
    
    # First do a dry run to count how many files would change
    print("Performing dry run...")
    dry_run_changes = set()
    for fix in fixes:
        if apply_fix_to_file(fix, {}, stats, dry_run=True, force=args.force, 
                           max_change_percentage=args.max_change_percentage):
            dry_run_changes.add(fix['file'])
    
    print(f"Dry run complete. {len(dry_run_changes)} files would be changed.")
    
    # Reset stats for actual run
    stats = FixStats()
    
    # Ask for confirmation in interactive mode
    if not args.no_interactive and dry_run_changes:
        try:
            confirmation = input(f"Apply changes to {len(dry_run_changes)} files? (y/n): ").lower()
            if confirmation != 'y':
                print("Operation cancelled by user")
                # Write to changes_count.txt for GitHub Actions
                with open('changes_count.txt', 'w') as f:
                    f.write("0")
                
                # Use the newer GitHub Actions output method
                if 'GITHUB_OUTPUT' in os.environ:
                    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                        f.write(f"has_changes=0\n")
                        f.write(f"changes_valid=false\n")
                sys.exit(0)
        except EOFError:
            print("Non-interactive environment detected, proceeding without confirmation")
    
    # Default to incremental mode unless batch mode is explicitly requested
    if not args.batch and not args.incremental:
        args.incremental = True
    
    # Incremental mode: apply and validate one fix at a time
    if args.incremental:
        print(f"Using incremental mode: applying and validating fixes every {args.validation_interval} changes")
        
        # Track fixes applied since last validation
        fixes_since_validation = 0
        files_to_validate = set()
        
        for fix in fixes:
            file_path = fix['file']
            
            # Apply this fix
            if apply_fix_to_file(fix, applied_fixes, stats, force=args.force, 
                               max_change_percentage=args.max_change_percentage):
                changed_files.add(file_path)
                files_to_validate.add(file_path)
                fixes_since_validation += 1
                
                # Validate files when we reach the interval or at the end
                if fixes_since_validation >= args.validation_interval or fix == fixes[-1]:
                    # Validate each modified file first
                    if not args.no_validation:
                        invalid_files = []
                        for validate_path in files_to_validate:
                            if not validate_single_file(validate_path, stats):
                                invalid_files.append(validate_path)
                        
                        # Then validate the whole project
                        project_valid = validate_project(stats)
                        
                        # If validation failed, revert files changed since last validation
                        if invalid_files or not project_valid:
                            print(f"Validation failed, reverting changes to {len(files_to_validate)} files")
                            for revert_path in files_to_validate:
                                revert_file(revert_path)
                                changed_files.remove(revert_path)
                                failed_fixes.append(fix)
                                stats.validation_failures += 1
                    
                    # Reset validation counters
                    fixes_since_validation = 0
                    files_to_validate = set()
    else:
        # Batch mode: apply all fixes then validate
        print("Using batch mode: applying all fixes before validation")
        
        # Apply all fixes
        for fix in fixes:
            file_path = fix['file']
            if apply_fix_to_file(fix, applied_fixes, stats, force=args.force,
                               max_change_percentage=args.max_change_percentage):
                changed_files.add(file_path)
        
        # Validate all changed files
        if not args.no_validation and changed_files:
            # First validate each file's syntax
            invalid_files = []
            for file_path in changed_files:
                if not validate_single_file(file_path, stats):
                    invalid_files.append(file_path)
            
            # Then validate the entire project
            project_valid = validate_project(stats)
            
            # If any validation failed, revert all changes
            if invalid_files or not project_valid:
                print("Validation failed, reverting all changes")
                for file_path in changed_files:
                    revert_file(file_path)
                    stats.validation_failures += len(fixes_by_file.get(file_path, []))
                changed_files = set()
    
    # Summary
    print("\n=== Summary ===")
    print(f"Total fixes found: {len(fixes)}")
    print(f"Files changed: {len(changed_files)}")
    print(f"Failed fixes: {len(failed_fixes)}")
    
    # Print detailed stats report
    print(stats.get_report())
    
    # Write output for GitHub Actions
    with open('changes_count.txt', 'w') as f:
        f.write(str(len(changed_files)))
    
    # Use the newer GitHub Actions output method
    if 'GITHUB_OUTPUT' in os.environ:
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"has_changes={1 if changed_files else 0}\n")
            f.write(f"changes_valid={len(failed_fixes) == 0}\n")
    
    # If we made changes, exit with code 1 to indicate changes were made
    # This can be used in CI to trigger a commit
    if changed_files:
        sys.exit(1)
    else:
        sys.exit(0)

if __name__ == "__main__":
    main()
